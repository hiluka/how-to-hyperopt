{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9a993c4",
   "metadata": {},
   "source": [
    "# The Role of Hyperparameters in Machine Learning Models and How to Tune Them (PSRM, 2023)\n",
    "### Christian Arnold, Luka Biedebach, Andreas KÃ¼pfer, and Marcel Neunhoeffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef42d16a",
   "metadata": {},
   "source": [
    "### *Code to replicate information depicted in Table 2, A4, A5, A6 and A7*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f22d43ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andreaskuepfer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#IMPORTS for Baselines\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=15)\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import EnglishStemmer, SpanishStemmer\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# IMPORTS for CNN\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import keras_tuner\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling1D, GlobalMaxPooling2D, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from keras_tuner import HyperModel, Objective\n",
    "from keras_tuner.tuners import RandomSearch, Hyperband\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, concatenate\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from gensim import models\n",
    "import gensim.downloader as api\n",
    "\n",
    "from utils.functions import *\n",
    "\n",
    "# Set to True if models should be run again\n",
    "rerun = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf84eeb",
   "metadata": {},
   "source": [
    "### Naive Bayes, Random Forest, and SVM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ec5ba28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerun flag true: tuning in progress...\n",
      "\n",
      "Current Country: Venezuela\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/vz-tweets_full.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# preprocess the data\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rerun:\n\u001b[0;32m---> 53\u001b[0m     data \u001b[38;5;241m=\u001b[39m preprocess_data(path, stops, stemmer)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# loop over seeds, load data and tune/train baseline models\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m seeds:\n",
      "File \u001b[0;32m~/Desktop/Data Science/MMDS/UML Hiwi Marcel_Chris/repo/repo_current/how-to-hyperopt/replication/utils/functions.py:57\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[0;34m(dataset_path, stops, stemmer)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# lowercase & join again\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tweet)\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m---> 57\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# map correct labels\u001b[39;00m\n\u001b[1;32m     60\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mCategorical(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviolence\u001b[39m\u001b[38;5;124m\"\u001b[39m], categories\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviolence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmalpractice\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcodes\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    571\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    572\u001b[0m     dialect,\n\u001b[1;32m    573\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    583\u001b[0m )\n\u001b[1;32m    584\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    479\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1038\u001b[0m     )\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:51\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     48\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# open handles\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Have to pass int, would break tests using TextReader directly otherwise :(\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py:222\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: FilePathOrBuffer, kwds: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    Let the readers open IOHandles after they are done with their potential raises.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/io/common.py:702\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    711\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/vz-tweets_full.csv'"
     ]
    }
   ],
   "source": [
    "if rerun:\n",
    "    print(\"Rerun flag true: tuning in progress...\")\n",
    "if not rerun:\n",
    "    print(\"Rerun flag false: results loaded from files...\")\n",
    "\n",
    "# set 5 different seeds for reproducibility\n",
    "seeds = [20210101, 20210102, 20210103, 20210104, 20210105]\n",
    "\n",
    "# initialize dictionary for countries/datasets\n",
    "countries = {\"Venezuela\": \"data/vz-tweets_full.csv\", \"Ghana\": \"data/gh-tweets_full.csv\", \"Philippines\": \"data/ph-tweets_full.csv\"}\n",
    "\n",
    "# define dataframe to store results\n",
    "results_svc = pd.DataFrame(\n",
    "    columns=[\"Baseline\", \"Country\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "results_svc_full = pd.DataFrame(\n",
    "    columns=[\"Baseline\", \"Country\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "\n",
    "results_randomforest = pd.DataFrame(\n",
    "    columns=[\"Baseline\", \"Country\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "results_randomforest_full = pd.DataFrame(\n",
    "    columns=[\"Baseline\", \"Country\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "\n",
    "results_naivebayes = pd.DataFrame(\n",
    "    columns=[\"Baseline\", \"Country\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "results_naivebayes_full = pd.DataFrame(\n",
    "    columns=[\"Baseline\", \"Country\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "\n",
    "results_tuning_svc = pd.DataFrame(columns=[\"Country\", \"kernel\", \"C\", \"class_weight\", \"gamma\", \"Tuning F1\", \"OOS F1\"])\n",
    "results_tuning_randomforest = pd.DataFrame(columns=[\"Country\", \"max_depth\", \"n_estimators\", \"class_weight\", \"max_features\", \"Tuning F1\", \"OOS F1\"])\n",
    "results_tuning_naivebayes = pd.DataFrame(columns=[\"Country\", \"alpha\", \"Tuning F1\", \"OOS F1\"])\n",
    "\n",
    "# loop over all countries\n",
    "\n",
    "for country, path in countries.items():\n",
    "    print(\"\\nCurrent Country: \" + country)\n",
    "    # initialize stopwords and stemmer in correct language\n",
    "    stops = set(stopwords.words(\"spanish\")) if country == \"Venezuela\" else set(stopwords.words(\"english\"))\n",
    "    stemmer = SpanishStemmer() if country == \"Venezuela\" else EnglishStemmer()\n",
    "    \n",
    "    results_scores_current_svc = []\n",
    "    results_scores_current_randomforest = []\n",
    "    results_scores_current_naivebayes = []\n",
    "    results_scores_current_untuned_svc = []\n",
    "    results_scores_current_untuned_randomforest = []\n",
    "    results_scores_current_untuned_naivebayes = []\n",
    "    \n",
    "    results_tuning_current_svc = []\n",
    "    results_tuning_current_randomforest = []\n",
    "    results_tuning_current_naivebayes = []\n",
    "    \n",
    "    # preprocess the data\n",
    "    if rerun:\n",
    "        data = preprocess_data(path, stops, stemmer)\n",
    "\n",
    "    # loop over seeds, load data and tune/train baseline models\n",
    "    for seed in seeds:\n",
    "        if rerun:\n",
    "            X_train_tfidf, X_test_tfidf, y_train, y_test = load_data(data, seed)\n",
    "\n",
    "        # SVC Tuned\n",
    "        print(\"SVC...\")\n",
    "        if rerun:\n",
    "            result_scores, results_tuning_current = run_svc(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "            results_scores_current_svc.append(result_scores)\n",
    "            results_tuning_svc = results_tuning_svc.append({\"Country\": country,\n",
    "                                                    \"kernel\": results_tuning_current[0],\n",
    "                                                    \"C\": results_tuning_current[1],\n",
    "                                                    \"class_weight\": results_tuning_current[2],\n",
    "                                                    \"gamma\": results_tuning_current[3],\n",
    "                                                    \"Tuning F1\": results_tuning_current[4],\n",
    "                                                    \"OOS F1\": result_scores[3]}, ignore_index=True)\n",
    "            print(result_scores)\n",
    "\n",
    "        # SVC Untuned\n",
    "        print(\"SVC Default...\")\n",
    "        if rerun:\n",
    "            result_scores = run_svc(X_train_tfidf, X_test_tfidf, y_train, y_test, tune = False)\n",
    "            results_scores_current_untuned_svc.append(result_scores)\n",
    "            print(result_scores)\n",
    "        \n",
    "        # Random Forest Tuned\n",
    "        print(\"Random Forest...\")\n",
    "        if rerun:\n",
    "            result_scores, results_tuning_current = run_randomforest(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "            results_scores_current_randomforest.append(result_scores)\n",
    "            results_tuning_randomforest = results_tuning_randomforest.append({\"Country\": country,\n",
    "                                                    \"max_depth\": results_tuning_current[0],\n",
    "                                                    \"n_estimators\": results_tuning_current[1],\n",
    "                                                    \"class_weight\": results_tuning_current[2],\n",
    "                                                    \"max_features\": results_tuning_current[3],\n",
    "                                                    \"Tuning F1\": results_tuning_current[4],\n",
    "                                                    \"OOS F1\": result_scores[3]}, ignore_index=True)\n",
    "            print(result_scores)\n",
    "        \n",
    "        # Random Forest Untuned  \n",
    "        print(\"Random Forest Default...\")\n",
    "        if rerun:\n",
    "            result_scores = run_randomforest(X_train_tfidf, X_test_tfidf, y_train, y_test, tune = False)\n",
    "            results_scores_current_untuned_randomforest.append(result_scores)\n",
    "            print(result_scores)\n",
    "        \n",
    "        # Naives Bayes Tuned\n",
    "        print(\"Naives Bayes...\")\n",
    "        if rerun:\n",
    "            result_scores, results_tuning_current = run_naivebayes(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "            results_scores_current_naivebayes.append(result_scores)\n",
    "            results_tuning_naivebayes = results_tuning_naivebayes.append({\"Country\": country,\n",
    "                                                                          \"alpha\": results_tuning_current[0],\n",
    "                                                                          \"Tuning F1\": results_tuning_current[1],\n",
    "                                                                          \"OOS F1\": result_scores[3]}, ignore_index=True)\n",
    "            print(result_scores)\n",
    "        \n",
    "        # Naives Bayes Untuned  \n",
    "        print(\"Naives Bayes Default...\")\n",
    "        if rerun:\n",
    "            result_scores = run_naivebayes(X_train_tfidf, X_test_tfidf, y_train, y_test, tune = False)\n",
    "            results_scores_current_untuned_naivebayes.append(result_scores)\n",
    "            print(result_scores)\n",
    "\n",
    "    if rerun:\n",
    "        # SVC Tuned\n",
    "        results_svc = results_svc.append({\"Baseline\": \"SVM Tuned\", \"Country\": country,\n",
    "                                  \"Accuracy\": np.array(results_scores_current_svc)[:, -4][0],\n",
    "                                  \"Precision\": np.array(results_scores_current_svc)[:, -3][0],\n",
    "                                  \"Recall\": np.array(results_scores_current_svc)[:, -2][0],\n",
    "                                  \"F1\": np.array(results_scores_current_svc)[:, -1][0]}, ignore_index=True)\n",
    "        results_svc_full = results_svc_full.append({\"Baseline\": \"SVM\", \"Country\": country,\n",
    "                              \"Accuracy\": results_scores_current_svc,\n",
    "                              \"Precision\": results_scores_current_svc,\n",
    "                              \"Recall\": results_scores_current_svc,\n",
    "                              \"F1\": results_scores_current_svc}, ignore_index=True)\n",
    "\n",
    "        # SVC Untuned                              \n",
    "        results_svc = results_svc.append({\"Baseline\": \"SVM Default\", \"Country\": country,\n",
    "                                  \"Accuracy\": np.array(results_scores_current_untuned_svc)[:, -4][0],\n",
    "                                  \"Precision\": np.array(results_scores_current_untuned_svc)[:, -3][0],\n",
    "                                  \"Recall\": np.array(results_scores_current_untuned_svc)[:, -2][0],\n",
    "                                  \"F1\": np.array(results_scores_current_untuned_svc)[:, -1][0]}, ignore_index=True)\n",
    "        results_svc_full = results_svc_full.append({\"Baseline\": \"SVM Untuned\", \"Country\": country,\n",
    "                              \"Accuracy\": results_scores_current_untuned_svc,\n",
    "                              \"Precision\": results_scores_current_untuned_svc,\n",
    "                              \"Recall\": results_scores_current_untuned_svc,\n",
    "                              \"F1\": results_scores_current_untuned_svc}, ignore_index=True)\n",
    "\n",
    "        # Random Forest Tuned\n",
    "        results_randomforest = results_randomforest.append({\"Baseline\": \"Random Forest Tuned\", \"Country\": country,\n",
    "                                  \"Accuracy\": np.array(results_scores_current_randomforest)[:, -4][0],\n",
    "                                  \"Precision\": np.array(results_scores_current_randomforest)[:, -3][0],\n",
    "                                  \"Recall\": np.array(results_scores_current_randomforest)[:, -2][0],\n",
    "                                  \"F1\": np.array(results_scores_current_randomforest)[:, -1][0]}, ignore_index=True)\n",
    "        results_randomforest_full = results_randomforest_full.append({\"Baseline\": \"Random Forest\", \"Country\": country,\n",
    "                              \"Accuracy\": results_scores_current_randomforest,\n",
    "                              \"Precision\": results_scores_current_randomforest,\n",
    "                              \"Recall\": results_scores_current_randomforest,\n",
    "                              \"F1\": results_scores_current_randomforest}, ignore_index=True) \n",
    "\n",
    "        # Random Forest Untuned\n",
    "        results_randomforest = results_randomforest.append({\"Baseline\": \"Random Forest Default\", \"Country\": country,\n",
    "                                  \"Accuracy\": np.array(results_scores_current_untuned_randomforest)[:, -4][0],\n",
    "                                  \"Precision\": np.array(results_scores_current_untuned_randomforest)[:, -3][0],\n",
    "                                  \"Recall\": np.array(results_scores_current_untuned_randomforest)[:, -2][0],\n",
    "                                  \"F1\": np.array(results_scores_current_untuned_randomforest)[:, -1][0]}, ignore_index=True)\n",
    "        results_randomforest_full = results_randomforest_full.append({\"Baseline\": \"Random Forest Untuned\", \"Country\": country,\n",
    "                              \"Accuracy\": results_scores_current_untuned_randomforest,\n",
    "                              \"Precision\": results_scores_current_untuned_randomforest,\n",
    "                              \"Recall\": results_scores_current_untuned_randomforest,\n",
    "                              \"F1\": results_scores_current_untuned_randomforest}, ignore_index=True) \n",
    "\n",
    "        # Naive Bayes Tuned\n",
    "        results_naivebayes = results_naivebayes.append({\"Baseline\": \"Naive Bayes Tuned\", \"Country\": country,\n",
    "                                  \"Accuracy\": np.array(results_scores_current_naivebayes)[:, -4][0],\n",
    "                                  \"Precision\": np.array(results_scores_current_naivebayes)[:, -3][0],\n",
    "                                  \"Recall\": np.array(results_scores_current_naivebayes)[:, -2][0],\n",
    "                                  \"F1\": np.array(results_scores_current_naivebayes)[:, -1][0]}, ignore_index=True)\n",
    "        results_naivebayes_full = results_naivebayes_full.append({\"Baseline\": \"Naive Bayes\", \"Country\": country,\n",
    "                              \"Accuracy\": results_scores_current_naivebayes,\n",
    "                              \"Precision\": results_scores_current_naivebayes,\n",
    "                              \"Recall\": results_scores_current_naivebayes,\n",
    "                              \"F1\": results_scores_current_naivebayes}, ignore_index=True)    \n",
    "\n",
    "        # Naive Bayes Untuned\n",
    "        results_naivebayes = results_naivebayes.append({\"Baseline\": \"Naive Bayes Default\", \"Country\": country,\n",
    "                                  \"Accuracy\": np.array(results_scores_current_untuned_naivebayes)[:, -4][0],\n",
    "                                  \"Precision\": np.array(results_scores_current_untuned_naivebayes)[:, -3][0],\n",
    "                                  \"Recall\": np.array(results_scores_current_untuned_naivebayes)[:, -2][0],\n",
    "                                  \"F1\": np.array(results_scores_current_untuned_naivebayes)[:, -1][0]}, ignore_index=True)\n",
    "        results_naivebayes_full = results_naivebayes_full.append({\"Baseline\": \"Naive Bayes\", \"Country\": country,\n",
    "                              \"Accuracy\": results_scores_current_untuned_naivebayes,\n",
    "                              \"Precision\": results_scores_current_untuned_naivebayes,\n",
    "                              \"Recall\": results_scores_current_untuned_naivebayes,\n",
    "                              \"F1\": results_scores_current_untuned_naivebayes}, ignore_index=True)\n",
    "\n",
    "if not rerun:\n",
    "    results_svc = pd.read_csv(\"results/svm_results.csv\")\n",
    "    results_randomforest = pd.read_csv(\"results/randomforest_results.csv\")\n",
    "    results_naivebayes = pd.read_csv(\"results/naivebayes_results.csv\")\n",
    "\n",
    "## Print and store detailed result scores\n",
    "print(\"--- Begin Table 2 ---\")\n",
    "print(results_naivebayes[[\"Baseline\", \"Country\", \"F1\"]].round(3))\n",
    "print(results_randomforest[[\"Baseline\", \"Country\", \"F1\"]].round(3))\n",
    "print(results_svc[[\"Baseline\", \"Country\", \"F1\"]].round(3))\n",
    "print(\"--- End Table 2 ---\")\n",
    "results_svc.round(3).to_csv(\"results/svm_results.csv\", index=False)\n",
    "results_randomforest.round(3).to_csv(\"results/randomforest_results.csv\", index=False)\n",
    "results_naivebayes.round(3).to_csv(\"results/naivebayes_results.csv\", index=False)\n",
    "\n",
    "if rerun:\n",
    "    results_svc_full.round(3).to_csv(\"results/svm_results_full.csv\", index=False)\n",
    "    results_randomforest_full.round(3).to_csv(\"results/randomforest_results_full.csv\", index=False)\n",
    "    results_naivebayes_full.round(3).to_csv(\"results/naivebayes_results_full.csv\", index=False)\n",
    "\n",
    "if not rerun:\n",
    "    results_tuning_svc = pd.read_csv(\"results/svm_results_hyperparameter.csv\")\n",
    "    results_tuning_randomforest = pd.read_csv(\"results/randomforest_results_hyperparameter.csv\")\n",
    "    results_tuning_naivebayes = pd.read_csv(\"results/naivebayes_results_hyperparameter.csv\")\n",
    "# Print and store best hyperparameter combinations (5 tuning runs) for each country\n",
    "print(\"--- Begin Table A4 ---\")\n",
    "print(results_tuning_naivebayes.round(3))\n",
    "print(\"--- End Table A4 ---\")\n",
    "print(\"--- Begin Table A5 ---\")\n",
    "print(results_tuning_svc.round(3))\n",
    "print(\"--- End Table A5 ---\")\n",
    "print(\"--- Begin Table A6 ---\")\n",
    "print(results_tuning_randomforest.round(3))\n",
    "print(\"--- End Table A6 ---\")\n",
    "\n",
    "results_tuning_svc.round(3).to_csv(\"results/svm_results_hyperparameter.csv\", index=False)\n",
    "results_tuning_randomforest.round(3).to_csv(\"results/randomforest_results_hyperparameter.csv\", index=False)\n",
    "results_tuning_naivebayes.round(3).to_csv(\"results/naivebayes_results_hyperparameter.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83692ec6",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1bf436",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "#load pre-trained model for correct language\n",
    "w2v_es = models.KeyedVectors.load_word2vec_format('sbw_vectors.bin', binary=True)\n",
    "w2v_en = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# initialize dictionary for countries/datasets\n",
    "countries = {\"Ghana\": \"raw/gh-tweets.csv\", \"Philippines\": \"raw/ph-tweets.csv\", \"Venezuela\": \"raw/vz-tweets.csv\"}\n",
    "\n",
    "#run several times with different param settings and seeds\n",
    "seeds = [20210101, 20210102, 20210103]\n",
    "\n",
    "#initialize result objects\n",
    "results_df = pd.DataFrame()\n",
    "results = []\n",
    "\n",
    "#loop over all countries\n",
    "for country, path in countries.items():\n",
    "\n",
    "    print(\"\\nCurrent Country: \" + country)\n",
    "\n",
    "    # initialize stopwords and stemmer in correct language\n",
    "    stops = set(stopwords.words(\"spanish\")) if country == \"Venezuela\" else set(stopwords.words(\"english\"))\n",
    "    stemmer = SpanishStemmer() if country == \"Venezuela\" else EnglishStemmer()\n",
    "    results_current = []\n",
    "\n",
    "    #load pre-trained model for correct language, to-do: aus der schleife holen\n",
    "    if country == \"Venezuela\":\n",
    "        w2v = w2v_es\n",
    "    else:\n",
    "        w2v = w2v_en\n",
    "\n",
    "    #preprocess the data\n",
    "    data = preprocess_data(path, stops, stemmer)\n",
    "\n",
    "    words=list(w2v.index_to_key)\n",
    "    vocab_len = len(w2v)\n",
    "    \n",
    "    i=1\n",
    "\n",
    "    for seed in seeds:\n",
    "\n",
    "        print(\"Run {i}/3\".format(i=i))\n",
    "\n",
    "        #iterate through different random train test splits to capture model variation\n",
    "        X_train_vec, X_train_tfidf, \\\n",
    "        X_test_vec, X_test_tfidf, \\\n",
    "        y_train_vec, y_test = embedding_transform(data, w2v,words, seed)\n",
    "        \n",
    "        tuner = tune_model_cv(X_train_vec, y_train_vec, model= country+str(seed), runs=2, epochs=200)\n",
    "        \n",
    "        #build model with best params\n",
    "        best_hp = tuner.get_best_hyperparameters()[0]\n",
    "        model = tuner.hypermodel.build(best_hp)\n",
    "        \n",
    "        #set class weight\n",
    "        ratio_1 = 1.0 - len(y_train_vec[y_train_vec == 1]) / float(len(y_train_vec))  ## ratio of violence instances\n",
    "        ratio_0 = 1.0 - ratio_1\n",
    "        class_weight = {0: ratio_0, 1: ratio_1}\n",
    "\n",
    "        #fit model\n",
    "        model.fit([X_train_vec, X_train_vec, X_train_vec], y_train_vec, epochs=200, batch_size=64, class_weight=class_weight)\n",
    "\n",
    "        #classify sequences\n",
    "        y_pred = model.predict([X_test_vec, X_test_vec, X_test_vec])\n",
    "        y_pred =(y_pred>0.5)\n",
    "\n",
    "        results.append(best_hp+[print_stats(y_test, y_pred, model = \"{c}_CNN_{p}\".format(p=\"tuned params\", c=country))])\n",
    "        pd.DataFrame(results, columns=[\"parameters\", \"model\", \"accuracy\", \"precision\", \"recall\", \"f1\"]).to_csv(\"temp_results_{c}_run_{p}.csv\".format(p=i, c=country))\n",
    "            \n",
    "        print(results)\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "#combine all results and calculate summary statistics\n",
    "results_df = pd.DataFrame(results, columns=[\"parameters\", \"model\", \"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
    "cnn_results.to_csv(\"results.csv\")\n",
    "cnn_results = results_df.groupby(results_df[\"model\"]).agg([np.mean, np.std])\n",
    "cnn_results.to_csv(\"final_results.csv\")\n",
    "\n",
    "print(cnn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef27087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
